{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.init import xavier_normal_\n",
    "import multiprocessing\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "Cutter = thulac.thulac(seg_only=True)\n",
    "\n",
    "\n",
    "ds = 256\n",
    "max_length = 512\n",
    "dc = 256\n",
    "learning_rate = 1e-3\n",
    "window_sizes = [2, 3, 4, 5]\n",
    "filter_num = 64\n",
    "dropout_rate = 0.5\n",
    "batch_size = 512\n",
    "epoch_num = 16\n",
    "embedding_dim = 200\n",
    "\n",
    "\n",
    "with open(\"law.txt\", 'r', encoding='utf-8') as f:\n",
    "    article_num = len(f.readlines())\n",
    "with open(\"accu.txt\", 'r', encoding='utf-8') as f:\n",
    "    accusation_num = len(f.readlines())\n",
    "penalty_num = 11\n",
    "\n",
    "\n",
    "def load_mapping(file_path):\n",
    "    id_to_name = {}\n",
    "    name_to_id = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            name = line.strip()\n",
    "            id_to_name[idx] = [name]\n",
    "            name_to_id[name] = idx\n",
    "    return id_to_name, name_to_id\n",
    "\n",
    "id_to_article, article_to_id = load_mapping(\"law.txt\")\n",
    "article_num = len(id_to_article)\n",
    "print(f\"article_num: {article_num}\")\n",
    "print(f\"Sample id_to_article: {dict(list(id_to_article.items())[:5])}\")\n",
    "print(f\"Sample article_to_id: {dict(list(article_to_id.items())[:5])}\")\n",
    "\n",
    "id_to_crime, crime_to_id = load_mapping(\"accu.txt\")\n",
    "accusation_num = len(id_to_crime)\n",
    "print(f\"accusation_num: {accusation_num}\")\n",
    "\n",
    "\n",
    "def load_w2v_matrix(numpy_path: str, w2id_path: str):\n",
    "    with open(w2id_path, 'rb') as f:\n",
    "        word2id_dict = pk.load(f)\n",
    "    array = np.load(numpy_path)\n",
    "    if not isinstance(array, np.ndarray):\n",
    "        raise TypeError(f\"Expected np.ndarray, got {type(array)}\")\n",
    "    word_embedding = torch.from_numpy(array.astype(np.float32))\n",
    "    return word_embedding, word2id_dict\n",
    "\n",
    "w, d = load_w2v_matrix(\"cail_thulac.npy\", \"w2id_thulac.pkl\")\n",
    "print(\"Model loaded succeed\")\n",
    "\n",
    "def transform_word2id(word):\n",
    "    return d.get(word, d[\"BLANK\"])\n",
    "\n",
    "\n",
    "def convert_imprisonment_to_term(tempterm):\n",
    "    if tempterm[\"death_penalty\"] == True or tempterm[\"life_imprisonment\"] == True or tempterm[\"imprisonment\"] == -1 or tempterm[\"imprisonment\"] == -2:\n",
    "        return 0\n",
    "    else:\n",
    "        imprisonment = tempterm[\"imprisonment\"]\n",
    "        if imprisonment > 10 * 12:\n",
    "            return 1\n",
    "        elif imprisonment > 7 * 12:\n",
    "            return 2\n",
    "        elif imprisonment > 5 * 12:\n",
    "            return 3\n",
    "        elif imprisonment > 3 * 12:\n",
    "            return 4\n",
    "        elif imprisonment > 2 * 12:\n",
    "            return 5\n",
    "        elif imprisonment > 1 * 12:\n",
    "            return 6\n",
    "        elif imprisonment > 9:\n",
    "            return 7\n",
    "        elif imprisonment > 6:\n",
    "            return 8\n",
    "        elif imprisonment > 0:\n",
    "            return 9\n",
    "        else:\n",
    "            return 10\n",
    "\n",
    "\n",
    "def map_term_to_months(term_idx):\n",
    "    term_to_months = {\n",
    "        0: 216,   # death_penalty or life_imprisonment -> 18 years (216 months)\n",
    "        1: 168,   # (120, 216) -> 168 months\n",
    "        2: 102,   # (84, 120] -> 102 months\n",
    "        3: 72,    # (60, 84] -> 72 months\n",
    "        4: 48,    # (36, 60] -> 48 months\n",
    "        5: 30,    # (24, 36] -> 30 months\n",
    "        6: 18,    # (12, 24] -> 18 months\n",
    "        7: 10.5,  # (9, 12] -> 10.5 months\n",
    "        8: 7.5,   # (6, 9] -> 7.5 months\n",
    "        9: 3,     # (0, 6] -> 3 months\n",
    "        10: 0     # 0\n",
    "    }\n",
    "    return term_to_months.get(term_idx, -3)  # -3 indicates an invalid prediction\n",
    "\n",
    "\n",
    "def load_jsonlines(file_path):\n",
    "    data = []\n",
    "    skipped_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            cleaned_line = line.strip().strip('\"')\n",
    "            if not cleaned_line:\n",
    "                continue\n",
    "            try:\n",
    "                item = json.loads(cleaned_line)\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning - Parsing error on line {line_num}: {e}, Line content: {cleaned_line}\")\n",
    "                skipped_lines.append((line_num, cleaned_line))\n",
    "                default_item = {\n",
    "                    \"meta\": {\n",
    "                        \"relevant_articles\": [],\n",
    "                        \"accusation\": [],\n",
    "                        \"term_of_imprisonment\": {\"imprisonment\": -3, \"death_penalty\": False, \"life_imprisonment\": False}\n",
    "                    }\n",
    "                }\n",
    "                data.append(default_item)\n",
    "    if skipped_lines:\n",
    "        print(f\"Skipped {len(skipped_lines)} lines of invalid data, but default values were filled to maintain the same number of lines\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_imprisonment_score(true_imprisonments, pred_imprisonments):\n",
    "    score_list = []\n",
    "    abstentions = 0\n",
    "    max_imprisonment = 216\n",
    "    \n",
    "    for true, pred in zip(true_imprisonments, pred_imprisonments):\n",
    "        true_term = true[0]\n",
    "        pred_term = pred[0]\n",
    "        \n",
    "        true_term = max_imprisonment if true_term in [-2, -1] else true_term\n",
    "        pred_term = max_imprisonment if pred_term in [-2, -1] else pred_term\n",
    "        \n",
    "        if pred_term == -3:\n",
    "            abstentions += 1\n",
    "            score_list.append(math.log(max_imprisonment))\n",
    "            continue\n",
    "            \n",
    "        if true_term < 0 or pred_term < 0:\n",
    "            continue\n",
    "        \n",
    "        score_list.append(abs(math.log(true_term + 1) - math.log(pred_term + 1)))\n",
    "    \n",
    "    if not score_list:\n",
    "        return {\"score\": 0, \"abstention_rate\": 1.0}\n",
    "    \n",
    "    log_distance = sum(score_list) / len(score_list)\n",
    "    normalized_score = (math.log(max_imprisonment) - log_distance) / math.log(max_imprisonment)\n",
    "    \n",
    "    return {\n",
    "        \"score\": normalized_score,\n",
    "        \"abstention_rate\": abstentions / len(true_imprisonments)\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_and_cache(file_path, cache_path):\n",
    "    global d\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached data from {cache_path}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data = pk.load(f)\n",
    "        for i, (fact, article, accusation, penalty) in enumerate(data[:5]):\n",
    "            print(f\"Cached sample {i}: article={article}, accusation={accusation}\")\n",
    "        return data\n",
    "    \n",
    "    print(f\"Generating cache for {file_path}\")\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=f\"Preprocessing {file_path}\"):\n",
    "            d_local = json.loads(line.strip())\n",
    "            fact = [transform_word2id(w[0]) if isinstance(w, list) and w else d[\"BLANK\"]\n",
    "                    for w in Cutter.cut(d_local['fact'])\n",
    "                    if isinstance(w, list) and w and w[0] not in [\",\", \".\", \"?\", \"\\\"\", \"”\", \"。\", \"？\", \"\", \"，\", \",\", \"、\", \"”\"]]\n",
    "            min_length = min(window_sizes)\n",
    "            if len(fact) < min_length:\n",
    "                fact.extend([d[\"BLANK\"]] * (min_length - len(fact)))\n",
    "            \n",
    "            \n",
    "            article_raw = d_local['meta']['relevant_articles']\n",
    "            article_labels = []\n",
    "            for art in article_raw:\n",
    "                art_str = str(art)\n",
    "                if art_str in article_to_id:\n",
    "                    article_labels.append(article_to_id[art_str])\n",
    "                else:\n",
    "                    print(f\"Warning: Article {art_str} not in article_to_id\")\n",
    "            if not article_labels:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            accusation_raw = d_local['meta']['accusation']\n",
    "            accusation_labels = []\n",
    "            for acc in accusation_raw:\n",
    "                acc_clean = re.sub(r\"[\\[\\]]\", \"\", acc).strip()\n",
    "                if acc_clean in crime_to_id:\n",
    "                    accusation_labels.append(crime_to_id[acc_clean])\n",
    "                else:\n",
    "                    print(f\"Warning: Accusation {acc_clean} not in crime_to_id\")\n",
    "            if not accusation_labels:\n",
    "                continue\n",
    "            \n",
    "            penalty = convert_imprisonment_to_term(d_local['meta']['term_of_imprisonment'])\n",
    "            data.append((fact, article_labels, accusation_labels, penalty))\n",
    "    \n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pk.dump(data, f)\n",
    "    print(f\"Data cached to {cache_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "class DataAdapterDataset(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        file_path = 'train_data94835.json' if mode == 'train' else 'test_data7050.json'\n",
    "\n",
    "        # cache\n",
    "        if mode == 'train':\n",
    "            cache_path = f\"{mode}94835_cache.pkl\"\n",
    "        if mode == 'test':\n",
    "            cache_path = f\"{mode}7050_cache.pkl\"\n",
    "        \n",
    "        self.data = preprocess_and_cache(file_path, cache_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    facts = [item[0] for item in batch]\n",
    "    max_len = min(max(len(f) for f in facts), max_length)\n",
    "    fact_tensor = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    for i, f in enumerate(facts):\n",
    "        fact_tensor[i, :min(len(f), max_len)] = torch.tensor(f[:min(len(f), max_len)])\n",
    "\n",
    "    \n",
    "    article_labels = torch.zeros(len(batch), article_num, dtype=torch.float)\n",
    "    for i, labels in enumerate([item[1] for item in batch]):\n",
    "        for lbl in labels:\n",
    "            article_labels[i, lbl] = 1.0\n",
    "\n",
    "    accusation_labels = torch.zeros(len(batch), accusation_num, dtype=torch.float)\n",
    "    for i, labels in enumerate([item[2] for item in batch]):\n",
    "        for lbl in labels:\n",
    "            accusation_labels[i, lbl] = 1.0\n",
    "\n",
    "    penalty_labels = torch.tensor([item[3] for item in batch], dtype=torch.long)\n",
    "    return fact_tensor, article_labels, accusation_labels, penalty_labels\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    DataAdapterDataset('train'), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=min(multiprocessing.cpu_count(), 4),\n",
    "    pin_memory=True,\n",
    "    drop_last=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    DataAdapterDataset('test'), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=min(multiprocessing.cpu_count(), 4), \n",
    "    pin_memory=True,\n",
    "    drop_last=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(dataloader.dataset))\n",
    "print(\"Test samples:\", len(test_dataloader.dataset))\n",
    "print(\"Train batches:\", len(dataloader))\n",
    "print(\"Test batches:\", len(test_dataloader))\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, filter_num, (k, embedding_dim)) for k in window_sizes])\n",
    "        for conv in self.convs:\n",
    "            nn.init.xavier_normal_(conv.weight)\n",
    "            nn.init.zeros_(conv.bias)\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        return x\n",
    "\n",
    "class MPBFNDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MPBFNDecoder, self).__init__()\n",
    "        self.S1 = nn.Parameter(xavier_normal_(torch.empty(article_num, ds)))\n",
    "        self.S2 = nn.Parameter(xavier_normal_(torch.empty(accusation_num, ds)))\n",
    "        self.S3 = nn.Parameter(xavier_normal_(torch.empty(penalty_num, ds)))\n",
    "        self.linear1 = nn.Linear(len(window_sizes) * filter_num, article_num)\n",
    "        self.linear2 = nn.Linear(len(window_sizes) * filter_num, accusation_num)\n",
    "        self.linear3 = nn.Linear(len(window_sizes) * filter_num, penalty_num)\n",
    "        self.Ws1 = nn.Parameter(xavier_normal_(torch.empty(dc, ds)))\n",
    "        self.Ws2 = nn.Parameter(xavier_normal_(torch.empty(dc, ds)))\n",
    "        self.Ws3 = nn.Parameter(xavier_normal_(torch.empty(dc, ds)))\n",
    "        self.Wf12 = nn.Parameter(xavier_normal_(torch.empty(accusation_num, ds)))\n",
    "        self.Wf13 = nn.Parameter(xavier_normal_(torch.empty(penalty_num, ds)))\n",
    "        self.Wf23 = nn.Parameter(xavier_normal_(torch.empty(penalty_num, ds)))\n",
    "        self.b12 = nn.Parameter(torch.zeros(accusation_num,))\n",
    "        self.b13 = nn.Parameter(torch.zeros(penalty_num,))\n",
    "        self.b23 = nn.Parameter(torch.zeros(penalty_num,))\n",
    "        self.Wg21 = nn.Parameter(xavier_normal_(torch.empty(article_num, ds)))\n",
    "        self.Wg31 = nn.Parameter(xavier_normal_(torch.empty(article_num, ds)))\n",
    "        self.Wg32 = nn.Parameter(xavier_normal_(torch.empty(accusation_num, ds)))\n",
    "        self.b21 = nn.Parameter(torch.zeros(article_num,))\n",
    "        self.b31 = nn.Parameter(torch.zeros(article_num,))\n",
    "        self.b32 = nn.Parameter(torch.zeros(accusation_num,))\n",
    "        self.elu = nn.ELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        nn.init.xavier_normal_(self.linear1.weight, gain=5.0)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_normal_(self.linear2.weight, gain=5.0)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        nn.init.xavier_normal_(self.linear3.weight, gain=5.0)\n",
    "        nn.init.zeros_(self.linear3.bias)\n",
    "\n",
    "    def normalize(self, x, epsilon=1e-8):\n",
    "        return x / (torch.sum(x, dim=-1, keepdim=True) + epsilon)\n",
    "\n",
    "    def forward(self, factori):\n",
    "        res1 = self.linear1(factori)\n",
    "        lsv1 = torch.matmul(res1, self.S1)\n",
    "        sem1 = torch.matmul(self.Ws1, lsv1.unsqueeze(2))\n",
    "        sem1 = self.elu(sem1)\n",
    "        fact1 = torch.mul(factori, sem1.squeeze(2))\n",
    "        pred12 = torch.matmul(self.Wf12, fact1.unsqueeze(2)).squeeze(2) + self.b12\n",
    "        pred13 = torch.matmul(self.Wf13, fact1.unsqueeze(2)).squeeze(2) + self.b13\n",
    "        res2 = self.normalize(pred12)\n",
    "        lsv2 = torch.matmul(res2, self.S2).unsqueeze(2)\n",
    "        gate21 = torch.matmul(self.Wg21, lsv2).squeeze(2) + self.b21\n",
    "        gate21 = self.sigmoid(gate21)\n",
    "        sem2 = torch.matmul(self.Ws2, lsv2)\n",
    "        sem2 = self.elu(sem2)\n",
    "        fact2 = torch.mul(factori, sem2.squeeze(2))\n",
    "        pred23 = torch.matmul(self.Wf23, fact2.unsqueeze(2)).squeeze(2) + self.b23\n",
    "        res3 = torch.mul(pred13, pred23)\n",
    "        res3 = self.softmax(res3)\n",
    "        lsv3 = torch.matmul(res3, self.S3).unsqueeze(2)\n",
    "        gate31 = torch.matmul(self.Wg31, lsv3).squeeze(2) + self.b31\n",
    "        gate31 = self.sigmoid(gate31)\n",
    "        gate32 = torch.matmul(self.Wg32, lsv3).squeeze(2) + self.b32\n",
    "        gate32 = self.sigmoid(gate32)\n",
    "        ver1 = self.normalize(torch.mul(gate21, gate31))\n",
    "        y1 = res1\n",
    "        ver2 = self.normalize(gate32)\n",
    "        y2 = pred12\n",
    "        y3 = res3\n",
    "        return y1, y2, y3\n",
    "\n",
    "class MPBFN(nn.Module):\n",
    "    def __init__(self, embedding: np.array, dropout_rate: float = dropout_rate):\n",
    "        super(MPBFN, self).__init__()\n",
    "        self.embs = nn.Embedding(164673, 200)\n",
    "        self.embs.weight.data.copy_(embedding)\n",
    "        self.embs.weight.requires_grad = False\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.decoder = MPBFNDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embs(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "model = MPBFN(w).to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion_multi = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([50.0]).to(device))\n",
    "criterion_single = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epoch_num}\")):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        facts_embedding = batch[0].to(device)\n",
    "        article_labels = batch[1].to(device)\n",
    "        accusation_labels = batch[2].to(device)\n",
    "        penalty_labels = batch[3].to(device)\n",
    "\n",
    "        o = model(facts_embedding)\n",
    "        loss1 = criterion_multi(o[0], article_labels)\n",
    "        loss2 = criterion_multi(o[1], accusation_labels)\n",
    "        loss3 = criterion_single(torch.log(o[2] + 1e-8), penalty_labels)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        loss.backward()\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {loss.item()}, article Loss: {loss1.item()}, accusation Loss: {loss2.item()}, penalty Loss: {loss3.item()}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name} grad norm: {param.grad.norm()}\")\n",
    "                break\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "penalty_strings = {\n",
    "    0: \"death_penalty or life_imprisonment\",\n",
    "    1: \"(10 years - ∞)\",\n",
    "    2: \"(7 years - 10 years]\",\n",
    "    3: \"(5 years - 7 years]\",\n",
    "    4: \"(3 years - 5 years]\",\n",
    "    5: \"(2 years - 3 years]\",\n",
    "    6: \"(1 year - 2 years]\",\n",
    "    7: \"(9 months - 12 months]\",\n",
    "    8: \"(6 months - 9 months]\",\n",
    "    9: \"(0 - 6 months]\",\n",
    "    10: \"0\"\n",
    "}\n",
    "\n",
    "\n",
    "article_labels_all = []\n",
    "article_preds_all = []\n",
    "accusation_labels_all = []\n",
    "accusation_preds_all = []\n",
    "penalty_labels_all = []\n",
    "penalty_preds_all = []\n",
    "\n",
    "\n",
    "test_data = load_jsonlines('test_data7050.json')\n",
    "true_imprisonments = []\n",
    "for item in test_data:\n",
    "    term = item['meta']['term_of_imprisonment']\n",
    "    if term['death_penalty']:\n",
    "        imprisonment = 216  # death_penalty\n",
    "    elif term['life_imprisonment']:\n",
    "        imprisonment = 216  # life_imprisonment\n",
    "    else:\n",
    "        imprisonment = term['imprisonment']\n",
    "    true_imprisonments.append([imprisonment])\n",
    "\n",
    "output_file = \"prediction_results.txt\"\n",
    "comparison_file = \"imprisonment_comparison.txt\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f, open(comparison_file, 'w', encoding='utf-8') as cf:\n",
    "    line_num = 0\n",
    "    pred_imprisonments = []\n",
    "\n",
    "    cf.write(\"Line\\tPred_Term_Index\\tPred_Term_Str\\tTrue_Term_Index\\tTrue_Term_Str\\tPred_Months\\tTrue_Months\\tMatch\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(test_dataloader, desc=\"Testing\")):\n",
    "            model.eval()\n",
    "            facts_embedding = batch[0].to(device)\n",
    "            article_labels = batch[1].to(device)\n",
    "            accusation_labels = batch[2].to(device)\n",
    "            penalty_labels = batch[3].to(device)\n",
    "            o = model(facts_embedding)\n",
    "            \n",
    "            article_preds_raw = torch.sigmoid(o[0])\n",
    "            accusation_preds_raw = torch.sigmoid(o[1])\n",
    "            k = 1\n",
    "            _, article_topk_indices = torch.topk(article_preds_raw, k, dim=1)\n",
    "            _, accusation_topk_indices = torch.topk(accusation_preds_raw, k, dim=1)\n",
    "            article_preds = torch.zeros_like(article_preds_raw)\n",
    "            accusation_preds = torch.zeros_like(accusation_preds_raw)\n",
    "            for i in range(article_preds.size(0)):\n",
    "                article_preds[i, article_topk_indices[i]] = 1.0\n",
    "                accusation_preds[i, accusation_topk_indices[i]] = 1.0\n",
    "            \n",
    "            \n",
    "            penalty_preds = torch.argmax(o[2], dim=1)\n",
    "            \n",
    "            \n",
    "            batch_pred_imprisonments = [[map_term_to_months(pred.item())] for pred in penalty_preds]\n",
    "            pred_imprisonments.extend(batch_pred_imprisonments)\n",
    "            \n",
    "            \n",
    "            batch_true_imprisonments = true_imprisonments[line_num:line_num + len(penalty_labels)]\n",
    "            for i in range(len(penalty_labels)):\n",
    "                true_penalty = penalty_labels[i].item()\n",
    "                pred_penalty = penalty_preds[i].item()\n",
    "                true_penalty_str = penalty_strings.get(true_penalty, \"Unknown\")\n",
    "                pred_penalty_str = penalty_strings.get(pred_penalty, \"Unknown\")\n",
    "                pred_months = map_term_to_months(pred_penalty)\n",
    "                true_months = batch_true_imprisonments[i][0]\n",
    "                penalty_match = 1 if true_penalty == pred_penalty else 0\n",
    "                cf.write(f\"{line_num + 1}\\t{pred_penalty}\\t{pred_penalty_str}\\t{true_penalty}\\t{true_penalty_str}\\t{pred_months}\\t{true_months}\\t{penalty_match}\\n\")\n",
    "                line_num += 1\n",
    "\n",
    "                \n",
    "                true_article = [int(idx) for idx, val in enumerate(article_labels[i]) if val == 1]\n",
    "                pred_article = [int(idx) for idx, val in enumerate(article_preds[i]) if val == 1]\n",
    "                true_article_str = [id_to_article.get(idx, [\"Unknown article\"])[0] for idx in true_article]\n",
    "                pred_article_str = [id_to_article.get(idx, [\"Unknown article\"])[0] for idx in pred_article]\n",
    "                article_match = 1 if set(true_article) == set(pred_article) else 0\n",
    "                f.write(f\"{line_num} (predict: article, ans: article)\\t{pred_article_str}\\t{true_article_str}\\t{article_match}\\n\")\n",
    "\n",
    "                true_accusation = [int(idx) for idx, val in enumerate(accusation_labels[i]) if val == 1]\n",
    "                pred_accusation = [int(idx) for idx, val in enumerate(accusation_preds[i]) if val == 1]\n",
    "                true_accusation_str = [id_to_crime.get(idx, [\"Unknown accusation\"])[0] for idx in true_accusation]\n",
    "                pred_accusation_str = [id_to_crime.get(idx, [\"Unknown accusation\"])[0] for idx in pred_accusation]\n",
    "                accusation_match = 1 if set(true_accusation) == set(pred_accusation) else 0\n",
    "                f.write(f\"{line_num} (predict: accusation, ans: accusation)\\t{pred_accusation_str}\\t{true_accusation_str}\\t{accusation_match}\\n\")\n",
    "\n",
    "                f.write(f\"{line_num} (predict: penalty, ans: penalty)\\t[{pred_penalty_str}]\\t[{true_penalty_str}]\\t{penalty_match}\\n\")\n",
    "\n",
    "            article_labels_all.extend(article_labels.cpu().numpy())\n",
    "            article_preds_all.extend(article_preds.cpu().numpy())\n",
    "            accusation_labels_all.extend(accusation_labels.cpu().numpy())\n",
    "            accusation_preds_all.extend(accusation_preds.cpu().numpy())\n",
    "            penalty_labels_all.extend(penalty_labels.cpu().numpy())\n",
    "            penalty_preds_all.extend(penalty_preds.cpu().numpy())\n",
    "\n",
    "\n",
    "imprisonment_metrics = calculate_imprisonment_score(true_imprisonments, pred_imprisonments)\n",
    "with open(output_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"\\nImprisonment Normalized Score: {imprisonment_metrics['score']:.3f}\\n\")\n",
    "    f.write(f\"Imprisonment Abstention Rate: {imprisonment_metrics['abstention_rate']:.3f}\\n\")\n",
    "\n",
    "\n",
    "true_articles_bin = np.array(article_labels_all)\n",
    "pred_articles_bin = np.array(article_preds_all)\n",
    "true_accusations_bin = np.array(accusation_labels_all)\n",
    "pred_accusations_bin = np.array(accusation_preds_all)\n",
    "true_imprisonments_bin = np.array(penalty_labels_all)\n",
    "pred_imprisonments_bin = np.array(penalty_preds_all)\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(true_bin, pred_bin, num_labels):\n",
    "    res = [{\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0} for _ in range(num_labels)]\n",
    "    for i in range(num_labels):\n",
    "        y_true = true_bin[:, i]\n",
    "        y_pred = pred_bin[:, i]\n",
    "        res[i][\"TP\"] = int((y_true * y_pred).sum())\n",
    "        res[i][\"FN\"] = int((y_true * (1 - y_pred)).sum())\n",
    "        res[i][\"FP\"] = int(((1 - y_true) * y_pred).sum())\n",
    "        res[i][\"TN\"] = int(((1 - y_true) * (1 - y_pred)).sum())\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_value(res):\n",
    "    if res[\"TP\"] == 0:\n",
    "        if res[\"FP\"] == 0 and res[\"FN\"] == 0:\n",
    "            precision = 1.0\n",
    "            recall = 1.0\n",
    "            f1 = 1.0\n",
    "        else:\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            f1 = 0.0\n",
    "    else:\n",
    "        precision = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FP\"]) if (res[\"TP\"] + res[\"FP\"]) > 0 else 0.0\n",
    "        recall = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FN\"]) if (res[\"TP\"] + res[\"FN\"]) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def gen_result(res):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    total = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "    for a in range(len(res)):\n",
    "        total[\"TP\"] += res[a][\"TP\"]\n",
    "        total[\"FP\"] += res[a][\"FP\"]\n",
    "        total[\"FN\"] += res[a][\"FN\"]\n",
    "        total[\"TN\"] += res[a][\"TN\"]\n",
    "        p, r, f = get_value(res[a])\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    macro_precision = sum(precision) / len(precision) if precision else 0.0\n",
    "    macro_recall = sum(recall) / len(recall) if recall else 0.0\n",
    "    macro_f1 = sum(f1) / len(f1) if f1 else 0.0\n",
    "\n",
    "    return macro_precision, macro_recall, macro_f1\n",
    "\n",
    "\n",
    "def imprisonment_multi_label_accuracy(true_labels, pred_labels):\n",
    "    correct = np.sum(np.all(true_labels == pred_labels, axis=1))\n",
    "    total = len(true_labels)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def multi_label_accuracy(true_labels, pred_labels):\n",
    "    correct = 0\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_set = set(np.where(true == 1)[0])\n",
    "        pred_set = set(np.where(pred == 1)[0])\n",
    "        if true_set == pred_set or true_set.issubset(pred_set) or pred_set.issubset(true_set):\n",
    "            correct += 1\n",
    "    total = len(true_labels)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "articles_acc = multi_label_accuracy(true_articles_bin, pred_articles_bin)\n",
    "accusations_acc = multi_label_accuracy(true_accusations_bin, pred_accusations_bin)\n",
    "imprisonments_acc = imprisonment_multi_label_accuracy(np.eye(penalty_num)[true_imprisonments_bin], np.eye(penalty_num)[pred_imprisonments_bin])\n",
    "\n",
    "res_articles = compute_confusion_matrix(true_articles_bin, pred_articles_bin, article_num)\n",
    "macro_p_a, macro_r_a, macro_f_a = gen_result(res_articles)\n",
    "res_accusations = compute_confusion_matrix(true_accusations_bin, pred_accusations_bin, accusation_num)\n",
    "macro_p_c, macro_r_c, macro_f_c = gen_result(res_accusations)\n",
    "res_imprisonments = compute_confusion_matrix(np.eye(penalty_num)[true_imprisonments_bin], np.eye(penalty_num)[pred_imprisonments_bin], penalty_num)\n",
    "macro_p_i, macro_r_i, macro_f_i = gen_result(res_imprisonments)\n",
    "\n",
    "\n",
    "# print(\"relevant_articles:\")\n",
    "print(\"Law_Articles:\")\n",
    "print(f\"  Accuracy: {articles_acc:.3f}\")\n",
    "print(f\"  Macro Precision: {macro_p_a:.3f}\")\n",
    "print(f\"  Macro Recall: {macro_r_a:.3f}\")\n",
    "print(f\"  Macro F1: {macro_f_a:.3f}\")\n",
    "\n",
    "# print(\"accusation:\")\n",
    "print(\"Charges:\")\n",
    "print(f\"  Accuracy: {accusations_acc:.3f}\")\n",
    "print(f\"  Macro Precision: {macro_p_c:.3f}\")\n",
    "print(f\"  Macro Recall: {macro_r_c:.3f}\")\n",
    "print(f\"  Macro F1: {macro_f_c:.3f}\")\n",
    "\n",
    "# print(\"imprisonment:\")\n",
    "print(\" Terms of Penalty:\")\n",
    "print(f\"  Accuracy: {imprisonments_acc:.3f}\")\n",
    "print(f\"  Macro Precision: {macro_p_i:.3f}\")\n",
    "print(f\"  Macro Recall: {macro_r_i:.3f}\")\n",
    "print(f\"  Macro F1: {macro_f_i:.3f}\")\n",
    "print(f\"  Normalized Score: {imprisonment_metrics['score']:.3f}\")\n",
    "print(f\"  Abstention Rate: {imprisonment_metrics['abstention_rate']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
